{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import dirname, join\n",
    "import numpy as np\n",
    "from transformers import AutoModelForMaskedLM,  AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "bert_model_tiny = 'google/bert_uncased_L-2_H-128_A-2'\n",
    "bert_model_mini = 'google/bert_uncased_L-4_H-256_A-4'\n",
    "bert_model_med = 'google/bert_uncased_L-8_H-512_A-8'\n",
    "bert_model_base = 'google/bert_uncased_L-12_H-768_A-12'\n",
    "longformer_model = 'allenai/longformer-base-4096'\n",
    "\n",
    "models_dict= {'mini': bert_model_mini, 'med': bert_model_med, 'tiny': bert_model_tiny, 'base':bert_model_base, 'longformer': longformer_model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(x, n):\n",
    "    ret = [x[0]] + x[-n:]\n",
    "    return ret\n",
    "\n",
    "class TorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length):\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.text[idx]\n",
    "        item_tokenized = self.tokenizer.batch_encode_plus([item], max_length= self.max_length, padding='max_length',\n",
    "                                                  truncation=False, return_special_tokens_mask=True, pad_to_max_length = True)\n",
    "\n",
    "        trucated_tokens = {key: truncate(val[0], self.max_length - 1) for key, val in item_tokenized.items()}\n",
    "        ret = {key: torch.tensor(val) for key, val in trucated_tokens.items()}\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name='tiny'\n",
    "max_len = 512\n",
    "model_name = models_dict[bert_model_name]\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=False, padding=True, max_len=max_len)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['My name is slim shade and I am an aspiring AI Engineer',\n",
    "'I am an aspiring AI Engineer',\n",
    "'My name is Slim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = text\n",
    "\n",
    "input_dataset = TorchDataset(train_data, tokenizer, max_length=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  8703,  1998,  1045,  2572,  2019, 22344,  9932,  3992,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'special_tokens_mask': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1045,  2572,  2019, 22344,  9932,  3992,   102,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0]),\n",
       " 'special_tokens_mask': tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2026,  2171,  2003, 11754,   102,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0]),\n",
       " 'special_tokens_mask': tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  8703,  1998,  1045,  2572,  2019, 22344,  9932,  3992,   102])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] shade and i am an aspiring ai engineer [SEP]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] my name is slim [SEP] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_dataset[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
