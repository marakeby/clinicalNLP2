{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_paper.file_browser_ import get_models\n",
    "from config_path import GCP_RESULTS_PATH, PLOTS_PATH\n",
    "from analysis.stat_utils import score_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_model_dirs(Task, tuned, models, size=None):\n",
    "    if size is None:\n",
    "        size = ['base', 'NA']\n",
    "    else:\n",
    "        size= size+[\"NA\"]\n",
    "    print (size)\n",
    "    if type(tuned) == list:\n",
    "        tuned.append('NA')\n",
    "    else:\n",
    "        tuned = [tuned, 'NA']\n",
    "\n",
    "    response_dirs = all_dirs[all_dirs.Task == Task]\n",
    "    response_dirs = response_dirs[response_dirs.Model.isin(models)].copy()\n",
    "    response_dirs = response_dirs[response_dirs.Tuned.isin(tuned)]\n",
    "    response_dirs = response_dirs[response_dirs.Size.isin(size)]\n",
    "    print(response_dirs)\n",
    "    print(response_dirs[['Frozen', 'Model', 'Size', 'Task', 'Tuned', 'classifier']])\n",
    "    # dirs = response_dirs.file\n",
    "    # print (dirs)\n",
    "    return response_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_predictions(dirs_df):\n",
    "    model_dict={}\n",
    "    for i, row in dirs_df.iterrows():\n",
    "        dir_ = row.file\n",
    "        # model = row.Model + '_' +row.Size\n",
    "        model = row.Model\n",
    "        dir_ = join(GCP_RESULTS_PATH, dir_)\n",
    "        prediction_file = [join(dir_,f) for f in listdir(dir_) if '0_testing.csv' in f][0]\n",
    "        pred_df = pd.read_csv(prediction_file)\n",
    "        print(pred_df.shape)\n",
    "        print(pred_df.head())\n",
    "        model_dict[model] = pred_df\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base', 'NA']\n",
      "   Frozen   Model  Size      Task Tuned classifier  \\\n",
      "8      NA     CNN    NA  response    NA        CNN   \n",
      "15   True    BERT  base  response  True        CNN   \n",
      "18     NA  TF-IDF    NA  response    NA        CNN   \n",
      "\n",
      "                                                 file  \n",
      "8   ./updated_labels/JAMA/response_one_split_sizes...  \n",
      "15  ./updated_labels/tuned_bert_cnn_frozen/respons...  \n",
      "18  ./updated_labels/tfidf/response_one_split_tfid...  \n",
      "   Frozen   Model  Size      Task Tuned classifier\n",
      "8      NA     CNN    NA  response    NA        CNN\n",
      "15   True    BERT  base  response  True        CNN\n",
      "18     NA  TF-IDF    NA  response    NA        CNN\n",
      "(1543, 7)\n",
      "   Unnamed: 0  DFCI_MRN ehr_scan_date              PROC_DESCR  pred  \\\n",
      "0          60  118571.0    2010-06-23  ct chest with contrast   0.0   \n",
      "1          61  118571.0    2010-11-15  ct chest with contrast   0.0   \n",
      "2          62  118571.0    2011-03-11  ct chest with contrast   0.0   \n",
      "3          63  118571.0    2011-07-22  ct chest with contrast   0.0   \n",
      "4          64  118571.0    2012-01-13  ct chest with contrast   0.0   \n",
      "\n",
      "   pred_score  y  \n",
      "0    0.012677  0  \n",
      "1    0.036252  0  \n",
      "2    0.004077  0  \n",
      "3    0.026359  0  \n",
      "4    0.046654  0  \n",
      "(1543, 7)\n",
      "   Unnamed: 0  DFCI_MRN ehr_scan_date              PROC_DESCR  pred  \\\n",
      "0          60  118571.0    2010-06-23  ct chest with contrast     0   \n",
      "1          61  118571.0    2010-11-15  ct chest with contrast     0   \n",
      "2          62  118571.0    2011-03-11  ct chest with contrast     0   \n",
      "3          63  118571.0    2011-07-22  ct chest with contrast     0   \n",
      "4          64  118571.0    2012-01-13  ct chest with contrast     0   \n",
      "\n",
      "   pred_score  y  \n",
      "0    0.009891  0  \n",
      "1    0.023329  0  \n",
      "2    0.005537  0  \n",
      "3    0.003705  0  \n",
      "4    0.032545  0  \n",
      "(1543, 7)\n",
      "   Unnamed: 0  DFCI_MRN ehr_scan_date              PROC_DESCR  pred  \\\n",
      "0          60  118571.0    2010-06-23  ct chest with contrast   0.0   \n",
      "1          61  118571.0    2010-11-15  ct chest with contrast   0.0   \n",
      "2          62  118571.0    2011-03-11  ct chest with contrast   0.0   \n",
      "3          63  118571.0    2011-07-22  ct chest with contrast   0.0   \n",
      "4          64  118571.0    2012-01-13  ct chest with contrast   0.0   \n",
      "\n",
      "   pred_score  y  \n",
      "0    0.024981  0  \n",
      "1    0.036312  0  \n",
      "2    0.029087  0  \n",
      "3    0.024976  0  \n",
      "4    0.024955  0  \n"
     ]
    }
   ],
   "source": [
    "task='response'\n",
    "models = ['BERT', 'CNN', 'TF-IDF']\n",
    "all_dirs= get_models()\n",
    "dirs_df = filter_model_dirs(Task=task, tuned=True, models=models)\n",
    "model_dict = read_predictions(dirs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores=[]\n",
    "names=[]\n",
    "xs=[]\n",
    "avg_scores=[]\n",
    "ci_uppers=[]\n",
    "ci_lowers=[]\n",
    "for i, k in enumerate(model_dict.keys()):\n",
    "    df = model_dict[k]\n",
    "    y_test = df['y']\n",
    "    y_pred_score = df['pred_score']\n",
    "    score, ci_lower, ci_upper, scores = score_ci(y_test, y_pred_score, score_fun=metrics.roc_auc_score,n_bootstraps=2000, seed=123)\n",
    "    all_scores.append(scores)\n",
    "    names.append(k)\n",
    "    ci_lowers.append(ci_lower)\n",
    "    ci_uppers.append(ci_upper)\n",
    "    xs.append(np.random.normal(i + 1, 0.04, len(scores)))\n",
    "    avg_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNN', 'BERT', 'TF-IDF']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_dict = dict(zip(names,all_scores))\n",
    "ci_lowers_dict= dict(zip(names, ci_lowers))\n",
    "ci_uppers_dict= dict(zip(names, ci_uppers))\n",
    "avg_scores_dict = dict(zip(names, avg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNN', 'BERT', 'TF-IDF']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BERT': 0.9014420769747576,\n",
       " 'CNN': 0.8868917748917748,\n",
       " 'TF-IDF': 0.8799734188430868}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci_lowers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BERT': 0.9468831168831169,\n",
       " 'CNN': 0.9390244975150637,\n",
       " 'TF-IDF': 0.93550892021244}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci_uppers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BERT': 0.9252129574601862,\n",
       " 'CNN': 0.9137145465664107,\n",
       " 'TF-IDF': 0.9079395019939193}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing CNN ,  BERT, pvalue=3.85310144457e-152 \n",
      "comparing CNN ,  TF-IDF, pvalue=2.10169366776e-44 \n",
      "comparing BERT ,  TF-IDF, pvalue=1.37168334573e-316 \n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for c1, c2 in itertools.combinations(all_scores_dict.keys(), 2):\n",
    "    scores1, scores2 = all_scores_dict[c1], all_scores_dict[c2]\n",
    "    twosample_results = stats.ttest_ind(scores1, scores2, equal_var=True)\n",
    "    pvalue = twosample_results[1]\n",
    "    print 'comparing {} ,  {}, pvalue={} '.format(c1, c2, pvalue)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9137145465664107, 0.9252129574601862, 0.9079395019939193]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.0, pvalue=1.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(scores1, scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:min_env]",
   "language": "python",
   "name": "conda-env-min_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
